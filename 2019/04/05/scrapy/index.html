<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">



<title>python scrapy框最最最基基础知识 | Love to share</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    


</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">zhhhy&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">zhhhy&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div id="post-toc" class="post-toc">
            <span class="post-toc-title">catalogue</span>
            <div class="post-toc-content">
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy框架的运行流程"><span class="toc-text">Scrapy框架的运行流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建一只爬虫"><span class="toc-text">创建一只爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提取数据"><span class="toc-text">提取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个实例来继续学习"><span class="toc-text">一个实例来继续学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li></ol>
            </div>
        </div>
    
    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">python scrapy框最最最基基础知识</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">zhhhy</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 5, 2019&nbsp;&nbsp;14:08:33</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <section class="post-content">
            <p>连续强刚了几天爬虫，大致摸清了这套框架的基本流程和最最最基本使用。</p>
<h2 id="Scrapy框架的运行流程"><a href="#Scrapy框架的运行流程" class="headerlink" title="Scrapy框架的运行流程"></a>Scrapy框架的运行流程</h2><p><img src="/.com//1.png" alt></p>
<p>很多教程都使用如上这个图。确实可以很清晰的看出该框架的运行机制。</p>
<ul>
<li>首先，当创建完一只Spider的时候，我们将启动这只蜘蛛。即给Spider开始运行。Spider会将<code>第一批url</code>发送给引擎<code>（Engine）</code> <strong>（当我们创建完一只蜘蛛时，会生成一批url，就是第一批URL）</strong>.</li>
<li>然后接着Engine就负责把这个安排给<code>Scheduler（调度器）</code>去处理。当处理完成的时候，<strong>（这里的处理只是生成request，并不是发包以后返回respond）</strong>,将生成的request再返回给引擎。</li>
<li>再接着，引擎把request传递给Downloader（下载器），由它进行将每个request的返回值（respond）下载回来。将接收到的返回值，传给引擎。</li>
<li>引擎再把这个返回值传给spider，spider进行处理（分类出item（数据）和继续爬取的url），依旧交还给引擎。</li>
<li>最后引擎将得到的item传给管道处理，将url给调度器进行处理。这样一次完整的爬去就结束了。</li>
</ul>
<p>从上述流程中，会发现引擎对数据的传递是原封不动的传递。</p>
<h2 id="创建一只爬虫"><a href="#创建一只爬虫" class="headerlink" title="创建一只爬虫"></a>创建一只爬虫</h2><p>在项目路径下使用该命令可以创建一只爬虫。</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject First_Spider #创建爬虫项目</span><br><span class="line">scrapy genspider baidu www.baidu.com #创建一个基本的爬虫 baidu为爬虫名字，www.baidu.com是要爬取的域名</span><br></pre></td></tr></table></figure>

<p>在spider文件夹里会生成一个baidu.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'baidu'</span>  <span class="comment">#爬虫名字</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.baidu.com'</span>]  <span class="comment"># 只爬去属于这些域名的url</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.baidu.com/'</span>] <span class="comment">#第一批爬取的url</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 启动爬虫</span><br><span class="line">scrapy crawl baidu</span><br></pre></td></tr></table></figure>

<h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>前面流程分析中，数据是由Spider进行分类，传给专门item管道处理用来保存。。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在items.py中设置需要接受的参数</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FirstSpiderItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    meta = scrapy.Field() <span class="comment"># 接受meta</span></span><br><span class="line">    title = scrapy.Field() <span class="comment"># 接受title</span></span><br></pre></td></tr></table></figure>

<p>那么回到Spider中的代码应该将数据分离出来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"><span class="comment"># 使用BF去解析返回值，其实并不需要，为了演示方便</span></span><br><span class="line">    soup = BeautifulSoup(response.body,<span class="string">'html.parser'</span>)</span><br><span class="line">    meta_all = soup.find_all(<span class="string">'meta'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> meta_all:</span><br><span class="line">        item = FirstSpiderItem()</span><br><span class="line">        item[<span class="string">'meta'</span>] = i.name <span class="comment">#仅仅是取到meta的标签名</span></span><br><span class="line">        print(<span class="string">"*"</span>*<span class="number">12</span>)</span><br><span class="line">        print(item[<span class="string">'meta'</span>])</span><br><span class="line">        print(<span class="string">"*"</span> * <span class="number">12</span>)</span><br><span class="line">        <span class="keyword">yield</span> item <span class="comment"># 将item返回item管道</span></span><br><span class="line">        <span class="comment"># yield作用是返回一个值，但并不把程序停止了。而return在返回的同时还把程序停止了</span></span><br><span class="line">        <span class="comment">#返回值是传递给了管道</span></span><br></pre></td></tr></table></figure>

<p>这里并不需要使用BF进行解析，response本身就支持使用CSS选择选择器和Xpath的方式提取数据。将获取到的数据保存起来使用如下命令</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl baidu -o result.json</span><br><span class="line"><span class="comment"># 保存成json格式</span></span><br><span class="line"><span class="comment"># json格式，默认为Unicode编码</span></span><br><span class="line"><span class="comment"># json lines格式，默认为Unicode编码</span></span><br><span class="line"><span class="comment"># csv 逗号表达式，可用Excel打开</span></span><br><span class="line"><span class="comment">#  xml格式</span></span><br></pre></td></tr></table></figure>

<h2 id="一个实例来继续学习"><a href="#一个实例来继续学习" class="headerlink" title="一个实例来继续学习"></a>一个实例来继续学习</h2><p>爬去腾讯招聘网站的数据。</p>
<p>先定义一下需要哪些数据。依旧是在item里定义需要获取到的字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TencentItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="comment"># 职位名</span></span><br><span class="line">    positionname = scrapy.Field()</span><br><span class="line">    <span class="comment"># 详情连接</span></span><br><span class="line">    positionlink = scrapy.Field()</span><br><span class="line">    <span class="comment"># 职位类别</span></span><br><span class="line">    positionType = scrapy.Field()</span><br><span class="line">    <span class="comment"># 招聘人数</span></span><br><span class="line">    peopleNum = scrapy.Field()</span><br><span class="line">    <span class="comment"># 工作地点</span></span><br><span class="line">    workLocation = scrapy.Field()</span><br><span class="line">    <span class="comment"># 发布时间</span></span><br><span class="line">    publishTime = scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>然后到spider去写分离数据的部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TencentpostionSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'tencentPostion'</span></span><br><span class="line">    allowed_domains = [<span class="string">'tencent.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://hr.tencent.com/position.php?&amp;start='</span>]</span><br><span class="line">    offset=<span class="number">0</span></span><br><span class="line">    base_url = <span class="string">"http://hr.tencent.com/position.php?&amp;start="</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.xpath(<span class="string">"//tr[@class='even'] | //tr[@class='odd']"</span>):</span><br><span class="line">            <span class="comment"># 初始化模型对象</span></span><br><span class="line">            item = TencentItem()</span><br><span class="line">            <span class="comment"># 职位名称</span></span><br><span class="line">            item[<span class="string">'positionname'</span>] = each.xpath(<span class="string">"./td[1]/a/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 详情连接</span></span><br><span class="line">            item[<span class="string">'positionlink'</span>] = each.xpath(<span class="string">"./td[1]/a/@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 职位类别</span></span><br><span class="line">            <span class="comment"># 这里有个坑，是在start=60的时候有一个职位类别是空的 导致程序报错。因此需要做一个判断</span></span><br><span class="line">            <span class="keyword">if</span> len(each.xpath(<span class="string">"./td[2]/text()"</span>))==<span class="number">0</span>:</span><br><span class="line">                item[<span class="string">'positionType'</span>] = <span class="string">''</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># print(each.xpath("./td[2]/text()").extract()[0])</span></span><br><span class="line">                item[<span class="string">'positionType'</span>] = each.xpath(<span class="string">"./td[2]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">                <span class="comment"># 招聘人数</span></span><br><span class="line">            item[<span class="string">'peopleNum'</span>] = each.xpath(<span class="string">"./td[3]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 工作地点</span></span><br><span class="line">            item[<span class="string">'workLocation'</span>] = each.xpath(<span class="string">"./td[4]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 发布时间</span></span><br><span class="line">            item[<span class="string">'publishTime'</span>] = each.xpath(<span class="string">"./td[5]/text()"</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">		<span class="comment"># 此处设置偏移量了，这种写死偏移量的方法其实不太合适，可以选用xpath选择器</span></span><br><span class="line">        <span class="keyword">if</span> self.offset &lt; <span class="number">3120</span>:</span><br><span class="line">            self.offset += <span class="number">10</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每次处理完一页的数据之后，重新发送下一页页面请求</span></span><br><span class="line">            <span class="comment"># self.offset自增10，同时拼接为新的url，并调用回调函数self.parse处理Response</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.base_url + str(self.offset), callback=self.parse)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#########################自动获取下一页的写法</span></span><br><span class="line">        <span class="comment">#判断是否存在下一页，不存在则停止爬取</span></span><br><span class="line">         <span class="keyword">if</span> len(response.xpath(<span class="string">"//a[@class='noactive'and @id='next']"</span>)):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment">#否则取出下一页的地址进行拼接</span></span><br><span class="line">        next = response.xpath(<span class="string">"//a[@id='next']/@href"</span>).extract()[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 每次处理完一页的数据之后，重新发送下一页页面请求</span></span><br><span class="line">            <span class="comment"># self.offset自增10，同时拼接为新的url，并调用回调函数self.parse处理Response</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(self.base_url + next, callback=self.parse) </span><br><span class="line">        <span class="comment"># 访问下一页</span></span><br></pre></td></tr></table></figure>

<p>如上代码，可以看出response本身就支持xpath语法进行数据提取，每一个xpath提取出来的数据都是一个列表。而且是select类型，需要使用extract()转换成字符串。利用yield的能力继续发送下一个请求，Request函数callback参数指的是定义调用哪个parse函数进行处理。</p>
<p>item类只是将数据存起来，实质上就是一个字典。前面提到item是给item管道进行处理，现在来编写管道文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line">class TencentPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.filename = open(&quot;result.json&quot;, &quot;w&quot;)</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        text = json.dumps(dict(item), ensure_ascii=False) + &quot;,\n&quot;</span><br><span class="line">        self.filename.write(text)</span><br><span class="line">        return item</span><br><span class="line">    def close_spider(self,spider):</span><br><span class="line">        self.filename.close()</span><br></pre></td></tr></table></figure>

<p><code>__init__</code>是管道类的构造函数。<code>close_spider</code>在爬虫关闭的时候执行。</p>
<p>当编写了管道，需要在setting中开启。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'Tencent.pipelines.TencentPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 后面的数字代表的是优先级，数字越小优先级越高。</span></span><br><span class="line"><span class="comment"># 也就是说可以有多个管道，而且会一个一个的按优先级执行下来。</span></span><br></pre></td></tr></table></figure>

<p>执行爬虫</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl tencentPostion</span><br></pre></td></tr></table></figure>

<p>发现不需要加-o参数了。就可以直接把文件保存出来。也就是说数据在管道里已经保存起来了。所以我们可以在管道里写各种的处理方法，使得完成一些数据分析。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>之前学习urllib、request、bs4这些库事已经可以做到提取数据，分析数据了。那么为什么还要用这个框架？是因为真的很方便。不需要自己构造发包，请求头参数等可以在setting里设置，并且可以随机选择一些请求头来逃避反爬虫机制。而回一下如果使用前面学习的类库，就得一步一步构造请求包。当然，前面学习的可以说是爬虫的原理，最基本的掌握了，使用这框架会更容易一些。</p>

        </section>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>zhhhy</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://yoursite.com/2019/04/05/scrapy/">http://yoursite.com/2019/04/05/scrapy/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Solgan:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</strong></strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/爬虫/"># 爬虫</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/06/28/zzzcms/">ZZZCMS V1.7.1 CVE</a>
            
            
            <a class="next" rel="next" href="/2019/04/02/spider/">爬虫学习之urllib、requests、BeautifulSoup</a>
            
        </section>


    </article>
</div>
        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© zhhhy | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Sirice19/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>
    </div>
</body>
</html>